{"cells":[{"cell_type":"markdown","source":["Name: Arjun Bhan  UNI: AB5666"],"metadata":{"id":"8ZbIwJFsA25x"},"id":"8ZbIwJFsA25x"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"080d50fc","outputId":"d625764d-c75d-4d32-a707-09f519b2a90f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting portalocker\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Installing collected packages: portalocker\n","Successfully installed portalocker-2.8.2\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.0\n"]}],"source":["!pip install portalocker\n","!pip install torchmetrics"],"id":"080d50fc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9XxyEQVWrO6"},"outputs":[],"source":["import argparse\n","import logging\n","import time\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","from torchtext.data import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.data.utils import get_tokenizer, ngrams_iterator\n","from torchtext.datasets import DATASETS\n","from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n","from torchtext.utils import download_from_url\n","from torchtext.vocab import build_vocab_from_iterator\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torchtext.vocab import GloVe\n","from tqdm import tqdm\n","\n","torch.autograd.set_detect_anomaly(True)\n"],"id":"B9XxyEQVWrO6"},{"cell_type":"markdown","metadata":{"id":"66eb271d"},"source":["### Information\n","- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n","- torchtext documentation: https://pytorch.org/text/stable/index.html"],"id":"66eb271d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c949153"},"outputs":[],"source":[],"id":"8c949153"},{"cell_type":"markdown","metadata":{"id":"12d93d22"},"source":["### Constants"],"id":"12d93d22"},{"cell_type":"code","execution_count":null,"metadata":{"id":"329c056d"},"outputs":[],"source":["DATASET = \"AG_NEWS\"\n","DATA_DIR = \".data\"\n","DEVICE = \"cpu\"\n","EMBED_DIM = 300\n","LR = 4.0\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 5\n","PADDING_VALUE = 0\n","PADDING_IDX = PADDING_VALUE"],"id":"329c056d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffada8d0"},"outputs":[],"source":[],"id":"ffada8d0"},{"cell_type":"markdown","metadata":{"id":"1a61aede"},"source":["### Get the tokenizer\n","- Use the WordLevel tokenizer.\n"],"id":"1a61aede"},{"cell_type":"code","execution_count":null,"metadata":{"id":"93e3b7cb"},"outputs":[],"source":["basic_english_tokenizer = get_tokenizer(\"basic_english\")\n"],"id":"93e3b7cb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aa4b78e4","outputId":"68e516c5-c97b-411a-81b5-c2a9a0de0f2c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this', 'is', 'some', 'text', '.', '.', '.']"]},"metadata":{},"execution_count":5}],"source":["basic_english_tokenizer(\"This is some text ...\")"],"id":"aa4b78e4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"505cf5ec"},"outputs":[],"source":["TOKENIZER = basic_english_tokenizer"],"id":"505cf5ec"},{"cell_type":"markdown","metadata":{"id":"64096cd8"},"source":["### Get the data and get the vocabulary"],"id":"64096cd8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce4a0578"},"outputs":[],"source":["def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield TOKENIZER(text)"],"id":"ce4a0578"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f48f23ab"},"outputs":[],"source":["train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n","VOCAB = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<pad>', '<unk>'))\n","\n","VOCAB.set_default_index(VOCAB['<unk>'])"],"id":"f48f23ab"},{"cell_type":"code","execution_count":null,"metadata":{"id":"552e7295"},"outputs":[],"source":[],"id":"552e7295"},{"cell_type":"markdown","metadata":{"id":"31ce9367"},"source":["### Get GloVe embeddings ... This will be slow ..."],"id":"31ce9367"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a43f0226","outputId":"792b71a6-93e5-43cc-bc7c-336cdcd1205f"},"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.840B.300d.zip: 2.18GB [06:49, 5.31MB/s]                            \n","100%|█████████▉| 2196016/2196017 [06:10<00:00, 5926.59it/s]\n"]}],"source":["GLOVE = GloVe()"],"id":"a43f0226"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HHPniGLnUYm"},"outputs":[],"source":[],"id":"4HHPniGLnUYm"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6302f433","outputId":"13af71da-1c5b-4e8c-b055-e79ba76f4455"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2196017, torch.Size([2196017, 300]))"]},"metadata":{},"execution_count":10}],"source":["len(GLOVE), GLOVE.vectors.shape"],"id":"6302f433"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1ba2b54"},"outputs":[],"source":[],"id":"c1ba2b54"},{"cell_type":"markdown","metadata":{"id":"200b05fc"},"source":["### Helper functions"],"id":"200b05fc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"16ca1ef5"},"outputs":[],"source":["def text_pipeline(text):\n","    return VOCAB(TOKENIZER(text))\n","\n","def label_pipeline(label):\n","    return int(label) - 1"],"id":"16ca1ef5"},{"cell_type":"markdown","metadata":{"id":"67ef6734"},"source":["Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"],"id":"67ef6734"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff479986"},"outputs":[],"source":["def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for (_label, _text) in batch:\n","        label_list.append(label_pipeline(_label))\n","\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text.clone().detach())\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","    text_list = pad_sequence(text_list, batch_first=True)\n","\n","    return label_list.to(DEVICE), text_list.to(DEVICE)"],"id":"ff479986"},{"cell_type":"markdown","metadata":{"id":"3iy4DLvGND70"},"source":[],"id":"3iy4DLvGND70"},{"cell_type":"markdown","metadata":{"id":"c7fcf425"},"source":["### Get the data"],"id":"c7fcf425"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e617ddce","outputId":"a106b04b-12db-4391-ffdb-f31852778301"},"outputs":[{"output_type":"stream","name":"stdout","text":["The number of classes is 4 ...\n"]}],"source":["train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n","num_class = len(set([label for (label, _) in train_iter]))\n","print(f\"The number of classes is {num_class} ...\")"],"id":"e617ddce"},{"cell_type":"markdown","metadata":{"id":"5aa8a40d"},"source":["### Set up the model"],"id":"5aa8a40d"},{"cell_type":"markdown","metadata":{"id":"8abf2ede"},"source":["Good reference on this type of model\n","- Recurrent CNN: https://ojs.aaai.org/index.php/AAAI/article/view/9513/9372"],"id":"8abf2ede"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dc51c359"},"outputs":[],"source":["class CNN1dTextClassificationModel(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size,\n","        num_class,\n","        embed_dim = 300,\n","        use_pretrained = True,\n","        fine_tune_embeddings = True\n","    ):\n","\n","        super(CNN1dTextClassificationModel, self).__init__()\n","\n","\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = PADDING_IDX)\n","\n","        if use_pretrained:\n","            self.embedding.weight.requires_grad = False\n","            for i in range(vocab_size):\n","                token = VOCAB.lookup_token(i)\n","                if token in GLOVE.stoi:\n","                    glove_index = GLOVE.stoi[token]\n","                    self.embedding.weight[i, :] = GLOVE.vectors[glove_index]\n","            self.embedding.weight.requires_grad = True\n","        else:\n","            self.init_weights()\n","\n","        if not fine_tune_embeddings:\n","            self.embedding.weight.requires_grad = False\n","\n","        self.cnn2 = nn.Conv1d(in_channels = embed_dim, out_channels=1, kernel_size= 2)\n","        self.cnn3 = nn.Conv1d(in_channels = embed_dim, out_channels=1, kernel_size= 3)\n","        self.cnn4 = nn.Conv1d(in_channels = embed_dim, out_channels=1, kernel_size= 4)\n","\n","        self.fc = nn.Linear(in_features = 3, out_features = num_class)\n","\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","        self.debug = False\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text):\n","\n","\n","        embedded = self.embedding(text)\n","\n","        if self.debug:\n","            print('embedding', embedded.shape)\n","\n","\n","        embedded = embedded.transpose(1, 2)\n","\n","\n","        cnn2 = self.cnn2(embedded)\n","        if self.debug:\n","            print('cnn2', cnn2.shape)\n","\n","\n","        cnn3 = self.cnn3(embedded)\n","        if self.debug:\n","            print('cnn3', cnn3.shape)\n","\n","\n","        cnn4 = self.cnn4(embedded)\n","        if self.debug:\n","            print('cnn4', cnn4.shape)\n","\n","        cnn2 = F.max_pool1d(cnn2, kernel_size = cnn2.size(2)).squeeze(2)\n","        cnn3 = F.max_pool1d(cnn3, kernel_size = cnn3.size(2)).squeeze(2)\n","        cnn4 = F.max_pool1d(cnn4, kernel_size = cnn4.size(2)).squeeze(2)\n","        if self.debug:\n","            print('cnn2 after max', cnn2.shape)\n","\n","\n","        cnn_concat = torch.cat((cnn2, cnn3, cnn4), dim = 1)\n","        cnn_concat = self.dropout(cnn_concat)\n","        if self.debug:\n","            print('cnn concat', cnn_concat.shape)\n","            self.debug = False\n","\n","        out = self.fc(cnn_concat)\n","\n","        return out\n","\n","class RecurrentCNNModel(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size,\n","        num_class = 4,\n","        e = 300,\n","        use_pretrained = True,\n","        fine_tune_embeddings = True,\n","\n","        debug = True\n","    ):\n","\n","        super(RecurrentCNNModel, self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, e)\n","\n","        self.c = 100\n","        self.h = 100\n","        self.initrange = 0.5\n","\n","        if use_pretrained:\n","            self.embedding.weight.requires_grad = False\n","\n","            for i in range(vocab_size):\n","                token = VOCAB.lookup_token(i)\n","\n","                if token in GLOVE.stoi:\n","                    glove_index = GLOVE.stoi[token]\n","                    self.embedding.weight[i, :] = GLOVE.vectors[glove_index]\n","            self.embedding.weight.requires_grad = True\n","        else:\n","            self.init_weights()\n","\n","        if not fine_tune_embeddings:\n","            self.embedding.weight.requires_grad = False\n","\n","\n","        self.Wl = nn.Linear(self.c, self.c)\n","        self.Wr = nn.Linear(self.c, self.c)\n","\n","        self.Wsl = nn.Linear(e, self.c)\n","        self.Wsr = nn.Linear(e, self.c)\n","\n","        self.W2 = nn.Linear(self.c * 2 + e, self.h)\n","        self.W4 = nn.Linear(self.h, num_class)\n","\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","        self.debug = False\n","\n","    def init_weights(self):\n","\n","      self.embedding.weight.data.uniform_(-self.initrange, self.initrange),\n","      self.W1.weight.data.uniform_(-self.initrange, self.initrange),\n","      self.Wr.weight.data.uniform_(-self.initrange, self.initrange),\n","      self.Wsl.weight.data.uniform_(-self.initrange, self.initrange),\n","      self.Wsr.weight.data.uniform_(-self.initrange, self.initrange)\n","\n","      self.W1.bias.data.zero_(),\n","      self.Wr.bias.data.zero_(),\n","      self.Wsl.bias.data.zero_(),\n","      self.Wsr.bias.data.zero_(),\n","      self.W2.bias.data.zero_(),\n","      self.W4.bias.data.zero_()\n","\n","\n","    def forward(self, text):\n","\n","        embedded = self.embedding(text)\n","\n","        N, L, D = embedded.size(0), embedded.size(1), embedded.size(2)\n","\n","        cr = torch.zeros(N, L, self.c)\n","\n","        if self.debug:\n","            print('cr ', cr.shape)\n","\n","        cl = torch.zeros(N, L, self.c)\n","\n","\n","        for l in range(1, L):\n","            cl[:, l, :] = self.relu(self.Wl(cl[: , l - 1, :].clone())+ self.Wsl(embedded[:, l - 1, :].clone()))\n","\n","\n","        for l in range(L-2, -1, -1):\n","            cr[:, l, :] = self.relu(self.Wr(cr[: , l + 1, :].clone()) + self.Wsr(embedded[:, l+1, :].clone()))\n","\n","\n","        x = torch.cat((cl, embedded, cr), dim = -1)\n","        if self.debug:\n","            print('x ', x.shape)\n","\n","\n","        y2 = torch.tanh(self.W2(x))\n","        if self.debug:\n","            print('y2 ', y2.shape)\n","\n","        y2 = y2.transpose(1, 2)\n","        if self.debug:\n","            print('y2 ', y2.shape)\n","\n","        y3 = torch.max(y2, dim = 2, keepdim = True)[0]\n","        y3 = y3.squeeze(2)\n","        if self.debug:\n","            print('y3 ', y3.shape)\n","\n","        y4 = self.W4(y3)\n","        if self.debug:\n","            print('y4 ', y4.shape)\n","            self.debug = False\n","\n","        return y4"],"id":"dc51c359"},{"cell_type":"code","execution_count":null,"metadata":{"id":"25775647"},"outputs":[],"source":[],"id":"25775647"},{"cell_type":"markdown","metadata":{"id":"3b3c6ed5"},"source":["### Set up the model"],"id":"3b3c6ed5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSpIDsg4dAEH"},"outputs":[],"source":["USE_PRETRANED = True,\n","\n","FINE_TUNE_EMBEDDINGS = True\n","\n","criterion = torch.nn.CrossEntropyLoss().to(DEVICE)"],"id":"zSpIDsg4dAEH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cef585f4"},"outputs":[],"source":["model =  RecurrentCNNModel(vocab_size = len(VOCAB), num_class = num_class)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n","\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size = 1.0)"],"id":"cef585f4"},{"cell_type":"markdown","metadata":{"id":"26266d8a"},"source":["### Set up the data"],"id":"26266d8a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9c0aebb5"},"outputs":[],"source":["train_iter, test_iter = DATASETS[DATASET]()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"],"id":"9c0aebb5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"85773616"},"outputs":[],"source":[],"id":"85773616"},{"cell_type":"markdown","metadata":{"id":"86476e2a"},"source":["### Train the model"],"id":"86476e2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"24950481"},"outputs":[],"source":["def train(dataloader, model, optimizer, criterion, epoch):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 100\n","\n","    for idx, (label, text) in tqdm(enumerate(dataloader)):\n","        optimizer.zero_grad()\n","        predicted_label = model(text)\n","\n","        loss = criterion(predicted_label, label)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            print(\n","                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n","                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count)\n","            )\n","            total_acc, total_count = 0, 0"],"id":"24950481"},{"cell_type":"code","execution_count":null,"metadata":{"id":"39a702be"},"outputs":[],"source":["def evaluate(dataloader, model):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text) in enumerate(dataloader):\n","            predited_label = model(text)\n","            total_acc += (predited_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc / total_count"],"id":"39a702be"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9e02c09"},"outputs":[],"source":["for epoch in range(1, NUM_EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader, model, optimizer, criterion, epoch)\n","    accu_val = evaluate(valid_dataloader, model)\n","    scheduler.step()\n","    print(\"-\" * 59)\n","    print(\n","        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n","        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n","    )\n","    print(\"-\" * 59)\n","\n","print(\"Checking the results of test dataset.\")\n","accu_test = evaluate(test_dataloader, model)\n","print(\"test accuracy {:8.3f}\".format(accu_test))"],"id":"a9e02c09"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuTptBdCblhJ"},"outputs":[],"source":["# Make a Conv Text model\n","model = CNN1dTextClassificationModel(vocab_size = len(VOCAB), num_class = num_class)\n","\n","# Set the optimizer to SGD\n","optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n","\n","# Set the scheduler to StepLR with gamma=0.1 and step_size = 1.0\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size = 1.0)"],"id":"WuTptBdCblhJ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f3b2754"},"outputs":[],"source":["# Train the Conv1d model\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader, model, optimizer, criterion, epoch)\n","    accu_val = evaluate(valid_dataloader, model)\n","    scheduler.step()\n","    print(\"-\" * 59)\n","    print(\n","        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n","        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n","    )\n","    print(\"-\" * 59)\n","\n","print(\"Checking the results of test dataset.\")\n","accu_test = evaluate(test_dataloader, model)\n","print(\"test accuracy {:8.3f}\".format(accu_test))"],"id":"9f3b2754"},{"cell_type":"markdown","metadata":{"id":"wGiUpurMIUK5"},"source":["\"Why do you think this CNN does not do very well on this data?\". Also, please explain why. (Hint: the answer is fairly short)"],"id":"wGiUpurMIUK5"},{"cell_type":"markdown","metadata":{"id":"_Yayw64-UAen"},"source":["The CNN does not do very well on this data as it doesn't apply nonlinearity to its model's architecture. This results in the model not being able to understand the data as well as the RNNCNN which applies nonlinearity. Nonlinearity is important as it allows the model to explore nonlinear patterns within the data."],"id":"_Yayw64-UAen"},{"cell_type":"code","execution_count":null,"metadata":{"id":"srcBg0-KlrP_"},"outputs":[],"source":[],"id":"srcBg0-KlrP_"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}
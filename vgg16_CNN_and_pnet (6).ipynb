{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Arjun Bhan (AB5666) and Leah Uzzan (lu2166)"
      ],
      "metadata": {
        "id": "dib6gEB9SXVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rspEZvmlu9oq"
      },
      "outputs": [],
      "source": [
        "# Function to load a single batch of CIFAR-10 data\n",
        "def load_cifar10_batch(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GIvc2KWu9ou"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 data\n",
        "data_folder = '/kaggle/input/Cifar10-ADL/cifar-10-batches-py/'  # Replace with your path\n",
        "batch_files = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'test_batch']\n",
        "#data_batches = [load_cifar10_batch(os.path.join(data_folder, f'{file}')) for file in batch_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0iBpB1eu9ou"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3Nism-Zu9ov",
        "outputId": "a9636135-c347-4551-a3e8-feb49cb45da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43228490.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with augmentation\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Splitting full training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(full_train_dataset))\n",
        "validation_size = len(full_train_dataset) - train_size\n",
        "train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, validation_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional VGGNet"
      ],
      "metadata": {
        "id": "cfusUaY5_0HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16Features, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # First block\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Second block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Third block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Fourth block\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Fifth block\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YQbcbUqo50A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16Classifier, self).__init__()\n",
        "        # The input feature size is 512 (from the conv layers) * 1 * 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RmRsjAWf5z4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedVGG16, self).__init__()\n",
        "        self.features = VGG16Features()\n",
        "        input_channels = 512  # Output channels of last conv layer in VGG16Features\n",
        "        self.classifier = VGG16Classifier(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = self.classifier(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "wZ1_9aJX6CRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to train and validate the model\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_loss = 0.0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ],
      "metadata": {
        "id": "FbJ7SfkK6CNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "id": "BP3RYnVm6CKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Loss Function, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModifiedVGG16(num_classes=10).to(device)\n",
        "# Use AdamW optimizer and StepLR scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "# Use a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "# Loss function?\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "DNq2bmCJ6K_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3B2kxpe6MXw",
        "outputId": "61462ac6-d4ba-4ae6-93fe-74611066bc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 2.0716, Training Accuracy: 17.53%, Validation Loss: 1.9309, Validation Accuracy: 23.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 1.7874, Training Accuracy: 29.07%, Validation Loss: 1.6865, Validation Accuracy: 35.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 1.5392, Training Accuracy: 40.95%, Validation Loss: 1.4256, Validation Accuracy: 44.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 1.3448, Training Accuracy: 48.95%, Validation Loss: 1.2618, Validation Accuracy: 52.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 1.1998, Training Accuracy: 55.51%, Validation Loss: 1.1706, Validation Accuracy: 57.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 1.0845, Training Accuracy: 60.87%, Validation Loss: 1.0619, Validation Accuracy: 62.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 0.9775, Training Accuracy: 65.28%, Validation Loss: 1.0785, Validation Accuracy: 62.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 0.9059, Training Accuracy: 68.11%, Validation Loss: 0.9320, Validation Accuracy: 67.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 0.8208, Training Accuracy: 71.18%, Validation Loss: 0.8371, Validation Accuracy: 70.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 0.7724, Training Accuracy: 72.98%, Validation Loss: 0.8307, Validation Accuracy: 71.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 0.7124, Training Accuracy: 75.26%, Validation Loss: 0.7975, Validation Accuracy: 72.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 0.6664, Training Accuracy: 76.92%, Validation Loss: 0.8151, Validation Accuracy: 71.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 0.6361, Training Accuracy: 78.23%, Validation Loss: 0.7226, Validation Accuracy: 75.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 0.5870, Training Accuracy: 79.78%, Validation Loss: 0.7241, Validation Accuracy: 75.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 0.5561, Training Accuracy: 80.92%, Validation Loss: 0.6921, Validation Accuracy: 77.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 0.5248, Training Accuracy: 82.02%, Validation Loss: 0.7286, Validation Accuracy: 76.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training Loss: 0.4968, Training Accuracy: 82.98%, Validation Loss: 0.7318, Validation Accuracy: 76.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training Loss: 0.4608, Training Accuracy: 84.48%, Validation Loss: 0.6579, Validation Accuracy: 78.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training Loss: 0.4359, Training Accuracy: 85.41%, Validation Loss: 0.6679, Validation Accuracy: 78.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training Loss: 0.4098, Training Accuracy: 86.25%, Validation Loss: 0.6872, Validation Accuracy: 78.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaDaCUU46CHG",
        "outputId": "46540b90-6cc9-4d23-c393-8004383f1888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYplyatc6CEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGGNet with PNet"
      ],
      "metadata": {
        "id": "2dHK4DDA6RyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbQ3KKhxu9ow"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PNet(nn.Module):\n",
        "    def __init__(self, input_channels, attention_channels):\n",
        "        super(PNet, self).__init__()\n",
        "        # Assuming the input feature map has a shape of [batch_size, input_channels, height, width]\n",
        "        # Convolutional layers to process feature maps\n",
        "        self.conv1 = nn.Conv2d(input_channels, attention_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(attention_channels, attention_channels, kernel_size=3, padding=1)\n",
        "        # Output layer to generate attention map with a single channel\n",
        "        self.output_layer = nn.Conv2d(attention_channels, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through convolutional layers with a non-linear activation function (e.g., ReLU)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # Generate the attention map\n",
        "        # We use a sigmoid function to ensure the output values are between 0 and 1\n",
        "        attention_map = torch.sigmoid(self.output_layer(x))\n",
        "        return attention_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAxPNJfZu9ow"
      },
      "outputs": [],
      "source": [
        "class VGG16Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16Features, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # First block\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Second block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Third block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Fourth block\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Fifth block\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4CnxKEeu9ox"
      },
      "outputs": [],
      "source": [
        "class VGG16Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16Classifier, self).__init__()\n",
        "        # The input feature size is 512 (from the conv layers) * 1 * 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9v-wfJru9ox"
      },
      "outputs": [],
      "source": [
        "class ModifiedVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10, attention_channels=64):\n",
        "        super(ModifiedVGG16, self).__init__()\n",
        "        self.features = VGG16Features()\n",
        "        input_channels = 512  # Output channels of last conv layer in VGG16Features\n",
        "        self.pnet = PNet(input_channels, attention_channels)\n",
        "        self.classifier = VGG16Classifier(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        attention_maps = self.pnet(features)\n",
        "        attended_features = features * attention_maps\n",
        "\n",
        "        # Flatten the attended features to match the classifier's input size\n",
        "        attended_features = attended_features.view(attended_features.size(0), -1)\n",
        "\n",
        "        output = self.classifier(attended_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0apeOHXu9oy"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to train and validate the model\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_loss = 0.0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Xo-PNFu9oy"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz-Zaej3u9oy"
      },
      "outputs": [],
      "source": [
        "# Model, Loss Function, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModifiedVGG16(num_classes=10).to(device)\n",
        "# Use AdamW optimizer and StepLR scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "# Use a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "# Loss function?\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXmlwJP_u9oy",
        "outputId": "e2eb53d5-f133-4881-8c8d-1266f0f43077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.80it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 2.1158, Training Accuracy: 16.39%, Validation Loss: 1.8863, Validation Accuracy: 24.55%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Training Loss: 1.7879, Training Accuracy: 29.90%, Validation Loss: 1.6566, Validation Accuracy: 36.72%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.13it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Training Loss: 1.5491, Training Accuracy: 40.42%, Validation Loss: 1.4346, Validation Accuracy: 45.33%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Training Loss: 1.3636, Training Accuracy: 48.92%, Validation Loss: 1.2683, Validation Accuracy: 51.97%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Training Loss: 1.2124, Training Accuracy: 55.75%, Validation Loss: 1.1672, Validation Accuracy: 57.40%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Training Loss: 1.0860, Training Accuracy: 61.41%, Validation Loss: 1.0711, Validation Accuracy: 61.95%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Training Loss: 0.9890, Training Accuracy: 65.13%, Validation Loss: 0.9820, Validation Accuracy: 65.39%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Training Loss: 0.9036, Training Accuracy: 68.32%, Validation Loss: 0.9510, Validation Accuracy: 65.85%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Training Loss: 0.8369, Training Accuracy: 70.55%, Validation Loss: 0.8440, Validation Accuracy: 69.85%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 0.7728, Training Accuracy: 72.73%, Validation Loss: 0.8404, Validation Accuracy: 69.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 0.7270, Training Accuracy: 74.94%, Validation Loss: 0.7639, Validation Accuracy: 73.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 0.6794, Training Accuracy: 76.37%, Validation Loss: 0.7393, Validation Accuracy: 73.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 22.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 0.6376, Training Accuracy: 77.94%, Validation Loss: 0.7797, Validation Accuracy: 73.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 0.5997, Training Accuracy: 79.47%, Validation Loss: 0.7035, Validation Accuracy: 75.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:27<00:00, 23.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 0.5550, Training Accuracy: 80.90%, Validation Loss: 0.7155, Validation Accuracy: 76.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 0.5237, Training Accuracy: 82.01%, Validation Loss: 0.6702, Validation Accuracy: 77.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training Loss: 0.4977, Training Accuracy: 83.25%, Validation Loss: 0.6912, Validation Accuracy: 77.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training Loss: 0.4681, Training Accuracy: 83.90%, Validation Loss: 0.6621, Validation Accuracy: 78.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training Loss: 0.4367, Training Accuracy: 85.17%, Validation Loss: 0.6559, Validation Accuracy: 78.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:26<00:00, 23.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training Loss: 0.4100, Training Accuracy: 86.11%, Validation Loss: 0.7205, Validation Accuracy: 76.90%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SPhVQAau9oz",
        "outputId": "28ad0460-955d-4c6a-a021-f99eeff48e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.53%\n"
          ]
        }
      ],
      "source": [
        "# Training and Evaluation\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCUrN8sOvo_U"
      },
      "source": [
        "VGGNet with PNet and updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7YfrrHru9o0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PNet(nn.Module):\n",
        "    def __init__(self, input_channels, attention_channels):\n",
        "        super(PNet, self).__init__()\n",
        "        # Assuming the input feature map has a shape of [batch_size, input_channels, height, width]\n",
        "        # Convolutional layers to process feature maps\n",
        "        self.conv1 = nn.Conv2d(input_channels, attention_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(attention_channels, attention_channels, kernel_size=3, padding=1)\n",
        "        # Output layer to generate attention map with a single channel\n",
        "        self.output_layer = nn.Conv2d(attention_channels, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through convolutional layers with a non-linear activation function (e.g., ReLU)\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope = 0.01)\n",
        "        x = F.leaky_relu(self.conv2(x), negative_slope = 0.01)\n",
        "        # Generate the attention map\n",
        "        # We use a sigmoid function to ensure the output values are between 0 and 1\n",
        "        attention_map = torch.sigmoid(self.output_layer(x))\n",
        "        return attention_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryrCWdJkvy6I"
      },
      "outputs": [],
      "source": [
        "class VGG16Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16Features, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # First block\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Second block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Third block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Fourth block\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Fifth block\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi4sAmVLvzkU"
      },
      "outputs": [],
      "source": [
        "class VGG16Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16Classifier, self).__init__()\n",
        "        # The input feature size is 512 (from the conv layers) * 1 * 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 1 * 1, 4096),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk_2ddNevxSf"
      },
      "outputs": [],
      "source": [
        "class ModifiedVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10, attention_channels=64):\n",
        "        super(ModifiedVGG16, self).__init__()\n",
        "        self.features = VGG16Features()\n",
        "        input_channels = 512  # Output channels of last conv layer in VGG16Features\n",
        "        self.pnet = PNet(input_channels, attention_channels)\n",
        "        self.classifier = VGG16Classifier(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        attention_maps = self.pnet(features)\n",
        "        attended_features = features * attention_maps\n",
        "\n",
        "        # Flatten the attended features to match the classifier's input size\n",
        "        attended_features = attended_features.view(attended_features.size(0), -1)\n",
        "\n",
        "        output = self.classifier(attended_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Gyw-i8u9o0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to train and validate the model\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_loss = 0.0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-IHth3Eu9o0"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H0FVlIau9o0"
      },
      "outputs": [],
      "source": [
        "# Model, Loss Function, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModifiedVGG16(num_classes=10).to(device)\n",
        "# Use AdamW optimizer and StepLR scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "# Use a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "# Loss function?\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j9dMeNnu9o0",
        "outputId": "639c3243-7d1d-4fce-f64a-b5429d5479bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.6963, Training Accuracy: 34.71%, Validation Loss: 1.5421, Validation Accuracy: 45.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 22.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 1.2788, Training Accuracy: 54.02%, Validation Loss: 1.6311, Validation Accuracy: 48.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 22.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 1.0922, Training Accuracy: 61.13%, Validation Loss: 1.0831, Validation Accuracy: 61.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 0.9652, Training Accuracy: 66.49%, Validation Loss: 0.8993, Validation Accuracy: 68.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:29<00:00, 21.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 0.8898, Training Accuracy: 69.20%, Validation Loss: 0.9435, Validation Accuracy: 67.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 0.8183, Training Accuracy: 71.65%, Validation Loss: 0.8342, Validation Accuracy: 71.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 0.7581, Training Accuracy: 74.01%, Validation Loss: 0.7640, Validation Accuracy: 73.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 0.7177, Training Accuracy: 75.45%, Validation Loss: 0.7575, Validation Accuracy: 74.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 0.6801, Training Accuracy: 76.92%, Validation Loss: 0.6771, Validation Accuracy: 77.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 0.6449, Training Accuracy: 78.30%, Validation Loss: 0.6313, Validation Accuracy: 78.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 0.6155, Training Accuracy: 79.13%, Validation Loss: 0.6077, Validation Accuracy: 78.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 0.5937, Training Accuracy: 79.80%, Validation Loss: 0.6032, Validation Accuracy: 79.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:29<00:00, 21.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 0.5710, Training Accuracy: 80.67%, Validation Loss: 0.5406, Validation Accuracy: 81.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:29<00:00, 21.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 0.5482, Training Accuracy: 81.38%, Validation Loss: 0.5646, Validation Accuracy: 80.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:29<00:00, 21.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 0.5262, Training Accuracy: 82.20%, Validation Loss: 0.5895, Validation Accuracy: 80.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 0.5057, Training Accuracy: 82.83%, Validation Loss: 0.5486, Validation Accuracy: 81.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training Loss: 0.4872, Training Accuracy: 83.56%, Validation Loss: 0.5472, Validation Accuracy: 81.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training Loss: 0.4723, Training Accuracy: 83.91%, Validation Loss: 0.5241, Validation Accuracy: 82.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training Loss: 0.4597, Training Accuracy: 84.43%, Validation Loss: 0.4939, Validation Accuracy: 83.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:28<00:00, 21.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training Loss: 0.4378, Training Accuracy: 85.15%, Validation Loss: 0.4953, Validation Accuracy: 82.63%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaBLYUw9wDCf",
        "outputId": "35cc652a-9830-41ba-df6f-eda529631024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.43%\n"
          ]
        }
      ],
      "source": [
        "# Training and Evaluation\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOjxQ5Ih358H"
      },
      "source": [
        "Traditional CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3zCE1q_0ewV"
      },
      "outputs": [],
      "source": [
        "class TradCNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn1 = nn.Conv2d(3, 30, kernel_size= 3, padding=1)\n",
        "    self.cnn2 = nn.Conv2d(30, 60, kernel_size= 3, padding=1)\n",
        "    self.cnn3 = nn.Conv2d(60, 90, kernel_size= 3, padding=1)\n",
        "    self.Linear1 = nn.Linear(90 * 4 *4, 480)\n",
        "    self.Linear2 = nn.Linear(480, 10)\n",
        "    self.maxPool = nn.MaxPool2d(kernel_size= 2, stride=2)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = self.cnn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = self.cnn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = torch.flatten(x, start_dim = 1)\n",
        "\n",
        "    x = self.Linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.Linear2(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCjqtm0G3VqX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to train and validate the model\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_loss = 0.0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        val_loss /= val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        scheduler.step(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KX4a2Jw3a2Q"
      },
      "outputs": [],
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1HKyYY84H3M"
      },
      "outputs": [],
      "source": [
        "# Model, Loss Function, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TradCNN().to(device)\n",
        "# Use AdamW optimizer and StepLR scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "# Use a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "# Loss function?\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOsZYwnu4HhK",
        "outputId": "9e1a4536-2285-4ae8-9032-e2e3a8b230a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 31.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.8036, Training Accuracy: 35.24%, Validation Loss: 1.5287, Validation Accuracy: 44.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:21<00:00, 29.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 1.4722, Training Accuracy: 46.84%, Validation Loss: 1.4209, Validation Accuracy: 48.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 1.3797, Training Accuracy: 50.47%, Validation Loss: 1.3506, Validation Accuracy: 51.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 1.3098, Training Accuracy: 53.48%, Validation Loss: 1.2768, Validation Accuracy: 54.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 1.2500, Training Accuracy: 55.42%, Validation Loss: 1.2386, Validation Accuracy: 56.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 1.2032, Training Accuracy: 57.45%, Validation Loss: 1.2139, Validation Accuracy: 56.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 1.1598, Training Accuracy: 59.05%, Validation Loss: 1.1451, Validation Accuracy: 59.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 1.1187, Training Accuracy: 60.44%, Validation Loss: 1.1244, Validation Accuracy: 59.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 1.0811, Training Accuracy: 61.95%, Validation Loss: 1.0798, Validation Accuracy: 61.42%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 1.0549, Training Accuracy: 62.90%, Validation Loss: 1.0757, Validation Accuracy: 62.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 1.0271, Training Accuracy: 63.87%, Validation Loss: 1.0386, Validation Accuracy: 62.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 1.0001, Training Accuracy: 65.02%, Validation Loss: 1.0190, Validation Accuracy: 63.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 0.9734, Training Accuracy: 66.02%, Validation Loss: 1.0180, Validation Accuracy: 64.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 0.9485, Training Accuracy: 66.78%, Validation Loss: 0.9682, Validation Accuracy: 65.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 0.9273, Training Accuracy: 67.67%, Validation Loss: 0.9657, Validation Accuracy: 66.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:19<00:00, 31.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 0.9108, Training Accuracy: 68.23%, Validation Loss: 0.9543, Validation Accuracy: 66.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 31.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training Loss: 0.8881, Training Accuracy: 69.03%, Validation Loss: 0.9396, Validation Accuracy: 67.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training Loss: 0.8701, Training Accuracy: 69.67%, Validation Loss: 0.9535, Validation Accuracy: 67.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training Loss: 0.8511, Training Accuracy: 70.51%, Validation Loss: 0.9036, Validation Accuracy: 68.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [00:20<00:00, 30.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training Loss: 0.8380, Training Accuracy: 70.97%, Validation Loss: 0.8863, Validation Accuracy: 68.75%\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEZCJ5Ru4L-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a32a96f-0370-4dc4-8359-744eb5d224ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 69.72%\n"
          ]
        }
      ],
      "source": [
        "# Training and Evaluation\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLt3FNhg6IRC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30588,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}